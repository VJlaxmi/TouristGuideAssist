{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlOOmO6Whb4O",
        "outputId": "cf205d1c-43b8-4845-8d5c-10671ef3e844"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.9/315.9 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q openai\n",
        "!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import userdata\n",
        "# userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jD_9g_nfhd9_",
        "outputId": "2e2773be-7caa-4240-c8af-cf9dfded4f15"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'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'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=userdata.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "\n",
        "assistant = client.beta.assistants.create(\n",
        "    name=\"TouristGuide Assist\",\n",
        "    instructions= \"You are a knowledgeable tour guide with extensive experience in providing detailed and engaging information about various destinations.\\\n",
        "    Known for your expertise in planning itineraries, sharing historical and cultural insights, and offering practical travel tips, you are here to assist users with any tour-related inquiries. \\\n",
        "    Your task is to answer questions about itinerary planning, historical and cultural information, travel advice, and other tour-related topics. You are friendly, informative, and passionate about travel, \\\n",
        "    striving to enhance the travel experience for your clients by sharing your extensive knowledge and providing valuable insights. If the user's query is outside the scope of tour guiding, please reply with - sorry, I can't answer.\",\n",
        "    model=\"gpt-3.5-turbo\"\n",
        ")\n",
        "\n",
        "import time\n",
        "\n",
        "def get_response(user_query):\n",
        "    try:\n",
        "        print(\"Creating thread...\")\n",
        "        thread = client.beta.threads.create()\n",
        "        print(f\"Created thread: {thread}\")\n",
        "\n",
        "        print(\"Creating message...\")\n",
        "        message = client.beta.threads.messages.create(\n",
        "            thread_id=thread.id,\n",
        "            role=\"user\",\n",
        "            content=user_query\n",
        "        )\n",
        "        print(f\"Created message: {message}\")\n",
        "\n",
        "        print(\"Creating run...\")\n",
        "        run = client.beta.threads.runs.create(\n",
        "            thread_id=thread.id,\n",
        "            assistant_id=assistant.id\n",
        "        )\n",
        "        print(f\"Created run: {run}\")\n",
        "\n",
        "        # Wait for the run to complete\n",
        "        while True:\n",
        "            print(\"Retrieving run status...\")\n",
        "            run = client.beta.threads.runs.retrieve(\n",
        "                thread_id=thread.id,\n",
        "                run_id=run.id\n",
        "            )\n",
        "            print(f\"Run status: {run.status}\")\n",
        "\n",
        "            if run.status == 'completed':\n",
        "                break\n",
        "            elif run.status in ['failed', 'cancelled']:\n",
        "                return f\"Run {run.status}.\"\n",
        "\n",
        "            print(\"Waiting for completion...\")\n",
        "            time.sleep(5)\n",
        "\n",
        "        print(\"Listing messages...\")\n",
        "        messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
        "        print(f\"List of messages: {messages}\")\n",
        "\n",
        "        for message in reversed(messages.data):\n",
        "            if message.role == \"assistant\" and message.content:  # Look for the assistant's message\n",
        "                # Extract text content from TextContentBlock\n",
        "                content = message.content[0].text.value if message.content else \"No content\"\n",
        "                print(f\"Assistant message found: {content}\")\n",
        "                return content  # Return the content of the assistant's message\n",
        "\n",
        "        return \"No response received.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return \"An error occurred.\"\n",
        "\n",
        "\n",
        "\n",
        "# Gradio Interface\n",
        "def gradio_interface(user_query):\n",
        "    response = get_response(user_query)\n",
        "    return response\n",
        "\n",
        "# Create Gradio Interface\n",
        "iface = gr.Interface(fn=gradio_interface, inputs=\"text\", outputs=\"text\", title=\"Tourist Guide Assist\", description=\"Ask me anything about Tourist Places!\")\n",
        "\n",
        "# Launch the Gradio Interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "iuucUglShfc2",
        "outputId": "a7126357-096f-4141-998a-f2c60ff5dc74"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://90701e86036d024fd2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://90701e86036d024fd2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ytj7YbwkjNNZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}